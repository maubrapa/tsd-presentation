<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Fast Traffic Sign Detection for Two-Way Roads using Detachable Onboard Cameras</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<!-- <link rel="stylesheet" href="dist/theme/black.css"> -->
		<!-- <link rel="stylesheet" href="dist/theme/white.css"> -->
		<link rel="stylesheet" href="dist/theme/white-contrast.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">

		<!-- Custom stylesheet -->
		<link rel="stylesheet" href="dist/custom.css">
		<!-- <link rel="stylesheet" href="dist/custom2.css"> -->

	</head>
 	<body>
		<div class="reveal">
			<div class="slides">
				<section data-state="my-style">
					<p>Fast Traffic Sign Detection for Two-Way Roads using Detachable Onboard Cameras</p>
					<br>
					<p><small>Maurı́cio B. de Paula$^\ast$ &nbsp;&nbsp; and &nbsp;&nbsp; Cláudio R. Jung$^\dagger$</small></p>
					<p style="font-size: medium;">
						$^\ast$Mathematics and Statistics Department, Federal University of Pelotas<br>
    					$^\dagger$Institute of Informatics, Federal University of Rio Grande do Sul
					<p style="margin-bottom: 4cm;"></p>
					<hr>
					<div class="twocolumn">
						<div><img src="./imgs/sibgrapi2023-logo.png" alt="SIBGRAPI 2023 logo" style="height: 65px; background: transparent;"></div>
						<div><img src="./imgs/logos_v2.svg" alt="UFPEL, UFRGS logo" style="height: 70px; background: transparent;"></div>
					</div>
					<!-- <p class="footnote"><font color="purple">Updated at October 20th, 2023</font></p> -->
					

				</section>
				<!-- ################################################## -->
				<section data-state="my-style">
					<h3>Outline</h3>
					<ul>
						<li>Motivation</li>
						<li>Related work</li>
						<li>Proposed approach</li>
						<li>Experimental results</li>
						<li>Conclusion</li>
					</ul>
				</section>
				<!-- ################################################## -->
				<!-- <section data-state="my-style" data-background-image="./imgs/crash-sketch-2.png"> -->
				<section data-state="my-style">
					<h3>Motivation</h3>
					<ul class="myli">
						<li>Approximately 1.35 million people die each year due to road traffic injuries worldwide<sup>1</sup>.</li>
						<li>Brazil has one of the highest rates of road traffic fatalities in the world.</li>
						<li>In 2022, there were approximately 20,852 road traffic deaths<sup>2</sup>.</li>
						<li>Campaigns have been carried out to reduce traffic injuries and deaths.</li>
						<li>Many vehicles are already equipped with ADAS, the aim of which is to reduce traffic injuries and increase traffic safety.</li>
					</ul>
					<p class="footnote"> 
						<sup>1</sup> World Health Organization (2018). Global status report on road safety 2018. Available at: https://www.who.int/publications/i/item/9789241565684.<br>
						<sup>2</sup>Registro Nacional de Acidentes e Estatísticas de Trânsito. Avaiable at: https://www.gov.br/transportes/pt-br/assuntos/transito/conteudo-Senatran/registro-nacional-de-acidentes-e-estatisticas-de-transito</p>
					<aside class="notes">
						(1) Advanced Driver Assistance Systems (ADAS): 
						which includes features like adaptive cruise control, lane-keeping assist, automatic emergency braking, and blind-spot monitoring. 
						(1.1) Many ADAS features rely on traffic sign detection to provide real-time information to drivers.
						(1.2) For example, adaptive cruise control systems can adjust a vehicle's speed based on speed limit signs, while lane-keeping assist systems can use traffic signs to understand road conditions and make necessary adjustments to the vehicle's trajectory.
						(1.3) These systems can help drivers avoid collisions and mitigate the severity of accidents.
						(2) Autonomous Vehicles: Self-driving cars have the potential to significantly reduce road traffic injuries by eliminating human error, which is a major cause of accidents. However, widespread adoption of this technology is still in progress and comes with its own set of challenges and safety considerations.
						(3) Crash Avoidance Technology: Automotive companies are investing in technology designed to prevent accidents, such as pedestrian detection systems, collision avoidance systems, and vehicle-to-vehicle (V2V) communication that allows cars to communicate with each other to prevent accidents.
					</aside>
				</section>
				<!-- ################################################## -->
				<section>
					<h3>Related work</h3>
					<ul class="myli">
						<li>Several TSD approaches are based on direct adaptations of generic-purpose object detectors.</li>
						<li>Runtime is still a problem when using low-power hardware.</li>
						<li>Most TSD approaches take as input the full image captured.</li>
						<li>The methods try to detect all traffic signs, without any previous knowledge of location or scale.</li>
						<li>Accuracy vs. Computational Demands</li>
					</ul>
					<aside class="notes">
						<li>Runtime Challenges</li>
						<li>To reach good accuracy (particularly for smaller signs), they need to work with higher input resolution and deeper networks, which leads to computationally demanding methods.</li>
					</aside>
				</section>
				<!-- ################################################## -->
				<section>
					<section data-state="my-style">
						<h3>The proposed approach</h3>
						<ul>
							<li>Main Contribution</li>
							<li><cinza>Definition of the ROIs</cinza></li>
							<li><cinza>Lightweight CNN for TSD</cinza></li>
							<li><cinza>Detection and Tracking mode</cinza></li>
						</ul>
					</section>
					<section data-background-color="rgb(255,255,255)">
						<h3>Main Contribution</h3>
						<p style="text-align:left; font-size: x-large;";>Define a small region of interest from the full frame by using the typical vertical traffic sign positions on rural roads, enabling the development of a low-cost TSD approach.</p>
						<figure>
							<img src="./imgs/roi_main_contribution_annotated.png" alt="Height and lateral location of sign in rural area" style="width: 640px; background: transparent;">
							<figcaption>Fig. 1. ROI (Region of interest)</figcaption>
						</figure>						
					</section>
				</section>
				<!-- ################################################## -->
				<section>
					<section data-state="my-style">
						<h3>The proposed approach</h3>
						<ul>
							<li><cinza>Main Contribution</cinza></li>
							<li>Definition of the ROIs</li>
							<li><cinza>Lightweight CNN for TSD</cinza></li>
							<li><cinza>Detection and Tracking mode</cinza></li>
						</ul>
					</section>					
					<section data-background-color="rgb(255,255,255)">
						<h3>Definition of the ROIs</h3>
						<br>
						<figure>
							<img src="./imgs/posicaoVia_ipe.svg" alt="Height and lateral location of sign in rural area" style="height: 200px; background: transparent;">
							<figcaption>Fig. 2. Height and lateral location of sign in rural area</figcaption>
						</figure>
						<aside class="notes">
							<li>Brazilian vertical traffic signs must be placed upright.</li>
							<li>The height and lateral distance of placement depend on the type of road.</li>
						</aside class="notes">
					</section>
					<section data-background-color="rgb(255,255,255)">
						<h3>Definition of the ROIs</h3>
						<br>
						<div class="twocolumn">
							<div><img src="./imgs/worldAndCameraCoordinates.svg" alt="3D world and camera coordinate" style="height: 250px; margin: 0 auto 4rem auto; background: transparent;"></div>
							<div>
								<br>
								<img src="./imgs/carro_frente.svg" alt="car front view" style="height: 100px; ">
							</div>
						</div>
						
						\( \mathbf{x}_w=(x,y,z)^T \color{gray}{\Longrightarrow} \mathbf{u}= {f}(\mathbf{x}_w) \color{gray}{\Longrightarrow} \mathbf{u}=(u,v)^T  \)

						<aside class="notes">
							<li>A detachable camera is placed at the top-central portion of the windshield, monitoring the road ahead.
							<li>we consider that there is no roll (since such movement is usually prevented by the windshield)
							<li>we estimate the remaining parameters using the online self-calibration scheme
							<li>With the fully calibrated camera, the next step is to identify the region in the WCS where traffic signs are expected to lie on, and then project it to the ICS using the known camera parameters.
							<li>Extrinsic camera parameters are characterized by the camera height and the pitch \( \alpha \) , yaw \( \beta \) and roll \( \gamma \) angles. </li>
							<li>given a 3D point xw, the corresponding pixel is defined by u = (u,v)^T is given by the perspective projection u=f(x), where f is based on the camera parameters</li>
						</aside>
					</section>
					<section data-state="my-style" data-background-color="rgb(255,255,255)">
						<h3>Definition of the ROIs</h3>
						<img src="./imgs/definicaoROI_new4.svg" alt="Region of interest in world coordinate system (left). Reprojection of the image regions (right)" style="height: 280px; background: transparent;">
						<p class="myli">Rectangular region \( r \) in WCS: \( \mathbf{w}_z = \left( p_h + \frac{p_d}{2}, \delta_y, \delta_z \right)^T \)</p>
						<small>							
							<ul>
								<li>$p_d$ and $p_h$ are the expected diameter and height.</li>								
								<li>$\delta_y$ is the approximate lateral distance (vehicle location $\to$ center of the sign)</li>
								<li>$\delta_z$ is the distance from the ROI to the camera.</li>
							</ul>
						</small>
						
						<small>
 						<aside class="notes">
 							<li> Region of interest in world coordinate system (left) </li>
 							<li>projection of the image regions (right)</li>
 							<li>In the WCS, the ROIs are rectangles orthogonal to both the ground plane and the central axis of the road.</li>
 							<li>We define a rectangular region r, at a distance δ z meters along the vertical axis in a birds-eye view</li>
 							<li>The region is defined by its width δ w (in meters) and height δ h (in meters), as well as its central position w z given by w_z = ...</li>
 							<li>p d and p h are the expected diameter and height</li>
							<li>ROIs are considerably smaller than the full-frame resolution.</li>
							<li>Present limited background variation.</li>
							<li>Shallower (and faster) CNNs suffice for TSD.</li>
						</aside>
						</small>
					</section>
				</section>
				<!-- ################################################## -->
				<section> <!-- vertical slides -->
					<section data-state="my-style"> 
						<h3>The proposed approach</h3>
						<ul>
							<li><cinza>Main Contribution<cinza></li>
							<li><cinza>Definition of the ROIs</cinza></li>
							<li>Lightweight CNN for TSD</li>
							<li><cinza>Detection and Tracking mode</cinza></li>
						</ul>
					</section>

					<section data-state="my-style" data-background-color="rgb(255,255,255)">
						<h3>Lightweight CNN for TSD</h3>
						<div class="twocolumn">
							<div>
								<!-- <img src="imgs/scap_arch.png" alt="CropNet structure" style="width:100%"> -->
								<img src="imgs/cropNet_structure.png" alt="CropNet structure" style="width:100%">
							</div>
							<div>
								<!-- <img src="imgs/img_forest.jpg" alt="Forest" style="width:100%"> -->
								<small>
									<table border=1>
										<thead>
											<tr>
												<th>Network</th>
												<th>Size</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>Proposed network</td>
												<td>394Kb</td>
											</tr>
											<tr>
												<td>Mobilenet-v2</td>
												<td>14Mb</td>
											</tr>
											<tr>
												<td>YOLOv4-tiny</td>
												<td>6Mb</td>
											</tr>
										</tbody>
									</table>
								</small>
							</div>
						</div>
						<img src="imgs/scap_arch.png" alt="CropNet structure" style="width:60%">
						<aside class="notes">
							<small>
							<li> input resolution presented a good compromise between running time and accuracy rates for ts located at 40m from the camera
							<li> the net has 6 convolutional and 4 maxpooling layers
							<li> the detection head is based on anchors (Single shot multibox detector and yolo)
							<li> Since the scale variation of the signs within the ROI is small we used a a limited nof anchors (one and three)
							<li> The size of the proposed network is smaller than generic purpose lightweight models/backbones
							</small>
						</aside>
					</section>
				</section>
				<!-- ################################################## -->
				<section>
					<section data-state="my-style"> 
						<h3>The proposed approach</h3>
						<ul>
							<li><cinza>Main Contribution</cinza></li>
							<li><cinza>Definition of the ROIs</cinza></li>
							<li><cinza>Lightweight CNN for TSD</cinza></li>
							<li>Detection and Tracking mode</li>
						</ul>
					</section>
					<section data-background-color="rgb(255,255,255)">
						<h3>Detection and tracking mode</h3>
								<img src="imgs/detAndTrackingMode.svg" alt="Detection mode" style="width:100%">
						<aside class="notes">
							<li>Explain modes: detection vs. tracking</li>
							<li>TS appears in the far field of the camera</li>
							<li>If a TS is detected in a given frame, it is expected to appear in the next frames</li>
						</aside class="notes">
					</section>
					<section data-background-color="rgb(255,255,255)">
						<h3>Detection and tracking mode</h3>						
								<video controls width="100%">
									<source src="videos/detectionMode.mp4" type="video/mp4" />
								</video>
								<!-- <img src="imgs/crop_farfield_resized_2.png" alt="Detection mode" style="width:100%"> -->
								<center> <small>Detection and tracking mode</small></center>
						<aside class="notes">
							<li>Explain modes: detection vs. tracking</li>
							<li>TS appears in the far field of the camera</li>
							<li>If a TS is detected in a given frame, it is expected to appear in the next frames</li>
						</aside class="notes">
					</section>
				</section>
				<!-- ################################################## -->
				<section>
					<section data-state="my-style"> 
						<h3>Experimental results</h3>
						<ul>
							<li>Dataset</li>
							<li><cinza>Training details</cinza></li>
							<li><cinza>Quantitative Evaluation</cinza></li>							
						</ul>
					</section>					
					<section>
						<h3>Our Dataset</h3>
						<div class="twocolumn">
							<div>
								<h4>Training set</h4>
								<small>
									<table>
										<thead>
											<tr>
												<th>Traffic sign</th>
												<th>#images</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>No overtaking</td>
												<td>939</td>
											</tr>
											<tr>
												<td>Left curve</td>
												<td>608</td>
											</tr>
											<tr>
												<td>Right curve</td>
												<td>744</td>
											</tr>
											<tr>
												<td>60 km/h</td>
												<td>189</td>
											</tr>
											<tr>
												<td>80 km/h</td>
												<td>195</td>
											</tr>
											<tr>
												<td>Trucks right</td>
												<td>167</td>
											</tr>
											<tr>
												<td>Bridge ahead</td>
												<td>102</td>
											</tr>
										</tbody>
									</table>

									<p>Total of 2,944 images</p>
								</small>
							</div>
							<div>
								<h4>Testing set</h4>
								<small>
									<ul>
										<li>Eleven (11) full HD videos</li>
										<li>Recorded with different placement settings.</li>
										<li>Extrinsic params. different for each video.</li>
										<li>Instrisic params. obteained offline.</li>
									</ul>									
								</small>
							</div>
						</div>
						<aside class="notes">
							(1) Although there are popular datasets for validating TSD algorithms, such as GTSDB [32], we are not aware of labeled datasets containing Brazilian traffic signs.
							(2) We need video sequences with known intrinsic camera parameters for the test stage.
							(3) We collected a set of images from Google Maps using street view API.
							(4) We recorded some video sequences with different TS.
							(5) Total of 2,944 images.
							(6) Recorded with different placement settings when attached to the windshield.
						</aside>
					</section>
					<section data-background-color="rgb(255,255,255)">
						<h3>Training details</h3>
						<div class="twocolumn">
							<div>
								<figure>
									<img src="./imgs/cropped_dataset.jpg" alt="cropped training images" style="width:100%">
									<figcaption>Fig. 3. Examples of the cropped training images</figcaption>
								</figure>
							</div>
							<div>
								<br>
								<small>
									<ul>
										<li>We cropped the training images around the annotade signs, randomly generating regions 3 to 5 times larger than the traffic sing.</li>
										<li>We used photometric augmentation and 2 different kernels to emulate defocus/motion blur.</li>
									</ul>
								</small>
								
							</div>
						</div>
						
						<aside class="notes">
							<li> To produce the training scenario (similar to the ROIs) we cropped the training images around the annotade signs, randomly generating regions 3 to 5 times larger than the traffic sing.</li>
							<li>We used photometric augmentation (hue, saturation, exposure shifts)</li>
							(2) The system was implemented in c++, with the OpenCV and darknet/caffe frameworks.
						</aside>
					</section>
					<section data-background-color="rgb(255,255,255)">
						<h3>Quantitative Evaluation</h3>
						<img src="./imgs/mAP_annotated.png" alt="mean average precision and processin times" style="width:100%">
					</section>
					<section>
						<video controls width="100%">
 						<!-- <video autoplay="true"> -->
							<source src="videos/vid5169.mp4" type="video/mp4" />
						</video>
						<center> <small>Since new traffic signs also appear, the detection mode is also active.</small></center>
					</section>
				</section>
				<!-- ################################################## -->
				<section>
					<h3>Conclusion</h3>
					<small>
						<ul>
							<li>Presented a framework for Brazilian TSD with flexible camera setup.</li>
							<li>Explored a calibrated camera to reduce the search area.</li>
							<li>Reduced the background complexity.</li>
							<li>The relative size of the TS presented small variations.</li>
							<li>Allowed the use of lightweight CNNs.
						</ul>
					</small>
				</section>
				<!-- ################################################## -->
				<!-- ################################################## -->
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: true,
				autoPlayMedia: true,
				// Parallax background image
				parallaxBackgroundImage: './imgs/bridge_1440_v3.png', 
				// e.g. "https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg"
				// parallaxBackgroundHorizontal: 200,
				// parallaxBackgroundVertical: 50,


				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealZoom, RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]


			});

			// Reveal.configure({ showNotes: true });

		</script>
	</body>
</html>
